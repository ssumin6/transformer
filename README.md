# Transformer
Implementation of Attention(NIPS'17) is all you need.

* Requirements: Python 3
* Dataset: Multi30k
* Features: Model, Framework agnostic

### References
[1] Transformer in DGL (https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer)

[2] Multi30k (https://www.aclweb.org/anthology/W16-3210/)

[3] Attention is all you need (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
